{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msotskaya/compling/blob/main/task1_basic_sotskaya_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHe6Z-d9h1Ay"
      },
      "source": [
        "### Задача 1: предобработка текста\n",
        "\n",
        "#### Цель:\n",
        "Научиться применять базовые методы предобработки текста с использованием библиотек NLTK и SpaCy\n",
        "\n",
        "#### Требования:\n",
        "1. Выполнить чистку данных\n",
        "2. Выполнить удаление стоп-слов\n",
        "3. Привести текст к нижнему регистру\n",
        "4. Произвести лемматизацию\n",
        "\n",
        "Инструкция содержит плейсходеры в скобках `< >`, которые вы должны заменить на код"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4PuMmgPimOc"
      },
      "source": [
        "**Шаг 1:** установите необходимые библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WL_siqOYizvz"
      },
      "outputs": [],
      "source": [
        "# Установите необходимые библиотеки (замените плейсхолдеры на ваш код)\n",
        "# Для NLTK:\n",
        "import nltk\n",
        "\n",
        "# Для SpaCy:\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lz2NPkDjrLy"
      },
      "source": [
        "**Шаг 2:** загрузите текст, откройте его и сохраните в переменную\n",
        "\n",
        "Ваш текст хранится по ссылке `https://github.com/vifirsanova/compling/blob/main/tasks/task1/data.txt`\n",
        "\n",
        "Это текст из блога про программирование\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue9FLkiTjGEy",
        "outputId": "4aced3d5-e087-4005-8f2f-14dd48cfba52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-26 22:18:24--  https://raw.githubusercontent.com/vifirsanova/compling/refs/heads/main/tasks/task1/data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 444304 (434K) [text/plain]\n",
            "Saving to: ‘data.txt’\n",
            "\n",
            "data.txt            100%[===================>] 433.89K  2.11MB/s    in 0.2s    \n",
            "\n",
            "2025-03-26 22:18:25 (2.11 MB/s) - ‘data.txt’ saved [444304/444304]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Загрузка текста с помощью командной строки\n",
        "!wget https://raw.githubusercontent.com/vifirsanova/compling/refs/heads/main/tasks/task1/data.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2nT0Zzjlw1F"
      },
      "outputs": [],
      "source": [
        "# Код для открытия файла\n",
        "with open('data.txt', 'r', encoding=\"UTF-8\") as file: # открываем файл при помощи конструкции with open() as и считываем содержимое в переменную text\n",
        "  text = file.read()\n",
        "text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M7U4ZuVjDxB"
      },
      "source": [
        "**Шаг 3:** Чистка текста  \n",
        "\n",
        "Удалите знаки препинания с помощью регулярных выражений. Закомментируйте код: объясните, какие регулярки вы применили, что они делают"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3z7qA4Nme8B"
      },
      "source": [
        "### Этап 1: очищаем текст от лишних символов\n",
        "\n",
        "Здесь, при помощи регулярных выражений, мы ищем все совпадения с паттерном, заменяя их на пустоту."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq5jWlo4mKxI"
      },
      "outputs": [],
      "source": [
        "# Код для удаления пунктуации\n",
        "import re\n",
        "cleaned_text = re.sub(r'[^\\w\\s]', '', text) #^ - это \"не\", \\w - любая буква и цифра, \\s - пробел. следовательно, удаляется всё, что не является буквой, цифрой или пробелом.\n",
        "print(\"Текст без пунктуации:\", cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3gq8ke1mSve"
      },
      "source": [
        "**Шаг 4:** Приведение текста к нижнему регистру\n",
        "\n",
        "### Этап 2: Приводим текст к нижнему регистру\n",
        "\n",
        "Это нужно, чтобы программа не считывала слова Word и word как разные, а воспринимала их как одно и то же. Для этого обращаемся к методу .lower() и применяем его к строке с нашим текстом."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkmK4-MDmcyZ"
      },
      "outputs": [],
      "source": [
        "# Код для приведения к нижнему регистру\n",
        "cleaned_text = cleaned_text.lower()\n",
        "print(\"Текст в нижнем регистре:\", cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bahI1VLpmq8u"
      },
      "source": [
        "**Шаг 5:** Удаление стоп-слов\n",
        "\n",
        "Удалите стоп-слова с помощью `NLTK`. Не забудьте загрузить дополнительные модули\n",
        "\n",
        "### Этап 3: Очищаем текст от стоп-слов\n",
        "\n",
        "Подгружаем список стоп-слов из nltk, превращаем его в множество. Проверяем на совпадения: если в нашем тексте (который мы предварительно превращаем в список методом .split()) встречается что-то из списка со стоп-словами, то это слово удаляем."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O4y6c4Fhzxc"
      },
      "outputs": [],
      "source": [
        "# Код для удаления стоп-слов с помощью NLTK\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in cleaned_text.split() if word not in stop_words]\n",
        "print(\"Текст без стоп-слов:\", filtered_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igP_3l8Bm-fk"
      },
      "source": [
        "**Шаг 6:** Лемматизация  \n",
        "\n",
        "Выполните лемматизацию текста с помощью SpaCy\n",
        "\n",
        "### Этап 4: Лемматизация\n",
        "\n",
        "Загружаем модель для английского языка, обрабатываем при помощи этой модели наш список очищенных слов и приводим к исходной форме. Результат сохраняем в список lemmatized_words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkQ6RbQ4nA8J"
      },
      "outputs": [],
      "source": [
        "# Код для лемматизации с помощью SpaCy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "lemmatized_words = [token.lemma_ for word in filtered_words for token in nlp(word)]\n",
        "print(\"Лемматизированный текст:\", lemmatized_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXAwtTdlnDlX"
      },
      "source": [
        "**Шаг 7:** Вывод результатов\n",
        "\n",
        "Представьте итоговый текст после всех этапов обработки.\n",
        "\n",
        "### Этап 5: Вывод результатов\n",
        "\n",
        "Чтобы текст выглядел красиво, с помощью метода join() превратим список обратно в строку. Итого - вывелся лемматизированный текст в нижнем регистре, очищенный от пунктуации и стоп-слов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaZ3BecgnJWk"
      },
      "outputs": [],
      "source": [
        "print(\"Итоговый текст:\", ' '.join(lemmatized_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd6i2ij3nO8N"
      },
      "source": [
        "**Шаг 8:** добавление описаний\n",
        "\n",
        "В каждой ячейке добавьте текстовые пояснения с помощью Markdown для описания шагов.\n",
        "\n",
        "Пример:\n",
        "\n",
        "```\n",
        "### Этап 1: Чистка текста\n",
        "Здесь мы удаляем пунктуацию, чтобы избавиться от ненужных символов.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d4qT98jnbmv"
      },
      "source": [
        "### Критерии оценивания:\n",
        "- **Чистка текста от артефактов:** текст очищен от пунктуации, символов (2 балла).\n",
        "- **Удаление стоп-слов:** все стоп-слова удалены (2 балла).\n",
        "- **Приведение текста к нижнему регистру:** весь текст преобразован в нижний регистр (2 балла).\n",
        "- **Лемматизация:** слова приведены к леммам (2 балла).\n",
        "- **Комментарии и описания:** каждый шаг кода содержит комментарии или текстовые описания (2 балла).\n",
        "\n",
        "Общий балл: **10 баллов**."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}